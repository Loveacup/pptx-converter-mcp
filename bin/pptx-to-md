#!/opt/homebrew/bin/python3.11
"""
ä¼˜åŒ–ç‰ˆ PPT è½¬ Markdown å·¥å…·
æ”¯æŒå¤šç§ Vision LLM åç«¯ + å¤šçº¿ç¨‹ + ç¼“å­˜
å¯ä½œä¸º MCP å·¥å…·ä½¿ç”¨

ç”¨æ³•:
    python pptx_to_markdown_enhanced.py <input.pptx> [output.md]
"""
import os
import sys
import json
import base64
import zipfile
import hashlib
import requests
import re
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Optional

# é…ç½®
MAX_WORKERS = int(os.environ.get("MAX_WORKERS", "3"))
CACHE_DIR = Path(os.environ.get("CACHE_DIR", "/tmp/ppt_image_cache"))

def load_config():
    """åŠ è½½ LLM é…ç½®ï¼ŒéªŒè¯å¿…éœ€çš„ç¯å¢ƒå˜é‡"""
    api_url = os.environ.get("LLM_API_URL")
    api_key = os.environ.get("LLM_API_KEY")
    model = os.environ.get("LLM_MODEL")

    if not api_url or not model:
        print("âŒ Error: LLM_API_URL and LLM_MODEL must be set", file=sys.stderr)
        print("", file=sys.stderr)
        print("Please copy .env.example to .env and configure your LLM settings,", file=sys.stderr)
        print("or set environment variables:", file=sys.stderr)
        print("  export LLM_API_URL=your-api-url", file=sys.stderr)
        print("  export LLM_API_KEY=your-api-key  # if required by your provider", file=sys.stderr)
        print("  export LLM_MODEL=your-model-name", file=sys.stderr)
        print("", file=sys.stderr)
        print("Examples:", file=sys.stderr)
        print("  # OpenAI", file=sys.stderr)
        print("  export LLM_API_URL=https://api.openai.com/v1/chat/completions", file=sys.stderr)
        print("  export LLM_API_KEY=sk-...", file=sys.stderr)
        print("  export LLM_MODEL=gpt-4o", file=sys.stderr)
        print("", file=sys.stderr)
        print("  # Local model (vLLM, Ollama, etc.)", file=sys.stderr)
        print("  export LLM_API_URL=http://localhost:8000/v1/chat/completions", file=sys.stderr)
        print("  export LLM_MODEL=your-model-name", file=sys.stderr)
        sys.exit(1)

    return api_url, api_key, model

API_URL, API_KEY, MODEL = load_config()

def ensure_cache_dir():
    CACHE_DIR.mkdir(parents=True, exist_ok=True)

def get_cache_path(pptx_path: Path) -> Path:
    """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
    pptx_hash = hashlib.md5(pptx_path.read_bytes()).hexdigest()[:16]
    return CACHE_DIR / f"{pptx_path.stem}_{pptx_hash}.json"

def load_cache(cache_path: Path) -> Dict:
    """åŠ è½½ç¼“å­˜"""
    if cache_path.exists():
        with open(cache_path, "r", encoding="utf-8") as f:
            return json.load(f)
    return {}

def save_cache(cache_path: Path, data: Dict):
    """ä¿å­˜ç¼“å­˜"""
    with open(cache_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def describe_image(image_path: Path) -> Optional[str]:
    """ä½¿ç”¨æœ¬åœ° LLM æè¿°å•å¼ å›¾ç‰‡"""
    try:
        with open(image_path, "rb") as f:
            img_data = f.read()
        
        img_base64 = base64.b64encode(img_data).decode('utf-8')
        
        payload = {
            "model": MODEL,
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": "è¯·ç”¨ä¸­æ–‡è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ã€‚å¦‚æœæ˜¯å›¾è¡¨ï¼Œè¯·è¯´æ˜å›¾è¡¨ç±»å‹ã€æ•°æ®è¶‹åŠ¿ï¼›å¦‚æœæ˜¯ç•Œé¢æˆªå›¾ï¼Œè¯·æè¿°ç•Œé¢å¸ƒå±€å’ŒåŠŸèƒ½ï¼›å¦‚æœæ˜¯å›¾æ ‡ï¼Œè¯·æè¿°å›¾æ ‡æ ·å¼å’Œå«ä¹‰ã€‚æ§åˆ¶åœ¨100å­—ä»¥å†…ã€‚"
                        },
                        {
                            "type": "image_url",
                            "image_url": {"url": f"data:image/{image_path.suffix[1:]};base64,{img_base64}"}
                        }
                    ]
                }
            ],
            "max_tokens": 200,
            "temperature": 0.3
        }
        
        headers = {"Content-Type": "application/json"}
        if API_KEY:
            headers["Authorization"] = f"Bearer {API_KEY}"
        
        response = requests.post(
            API_URL,
            headers=headers,
            json=payload,
            timeout=60
        )
        response.raise_for_status()
        result = response.json()
        return result['choices'][0]['message']['content'].strip()
        
    except Exception as e:
        return f"[å›¾ç‰‡æè¿°ç”Ÿæˆå¤±è´¥: {e}]"

def extract_images_from_pptx(pptx_path: Path, extract_dir: Path) -> List[Path]:
    """ä» PPT ä¸­æå–æ‰€æœ‰å›¾ç‰‡"""
    images = []
    supported_ext = {'.png', '.jpg', '.jpeg', '.gif', '.bmp', '.webp'}
    
    try:
        with zipfile.ZipFile(pptx_path, 'r') as z:
            media_files = [f for f in z.namelist() if f.startswith('ppt/media/')]
            
            for media_file in media_files:
                file_ext = Path(media_file).suffix.lower()
                if file_ext not in supported_ext:
                    continue
                    
                file_data = z.read(media_file)
                filename = Path(media_file).name
                output_path = extract_dir / filename
                
                with open(output_path, 'wb') as f:
                    f.write(file_data)
                
                images.append(output_path)
    
    except Exception as e:
        print(f"âš ï¸  æå–å›¾ç‰‡å¤±è´¥: {e}", file=sys.stderr)
    
    return images

def process_images_batch(images: List[Path], cache: Dict) -> Dict[str, str]:
    """æ‰¹é‡å¤„ç†å›¾ç‰‡ï¼ˆå¸¦è¿›åº¦æ˜¾ç¤ºï¼‰"""
    descriptions = {}
    completed = 0
    failed = 0
    
    def process_single(img_path: Path) -> tuple:
        img_id = img_path.stem
        
        # æ£€æŸ¥ç¼“å­˜
        if img_id in cache:
            return img_id, cache[img_id], True
        
        # ç”Ÿæˆæè¿°
        desc = describe_image(img_path)
        if desc.startswith("[å›¾ç‰‡æè¿°ç”Ÿæˆå¤±è´¥"):
            return img_id, desc, False
        
        return img_id, desc, True
    
    print(f"ğŸ”„ å¼€å§‹è¯†åˆ« {len(images)} å¼ å›¾ç‰‡...")
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_image = {executor.submit(process_single, img): img for img in images}
        
        for future in as_completed(future_to_image):
            img_path = future_to_image[future]
            try:
                img_id, description, success = future.result()
                descriptions[img_id] = description
                
                if success:
                    completed += 1
                    cache[img_id] = description
                else:
                    failed += 1
                
                progress = (completed + failed) / len(images) * 100
                print(f"\r   â³ è¿›åº¦: {completed+failed}/{len(images)} ({progress:.1f}%) - {img_id}", 
                      end='', flush=True)
                
            except Exception as e:
                failed += 1
                print(f"\n   âŒ å¤±è´¥: {img_path.name} - {e}", file=sys.stderr)
    
    print(f"\n   âœ… å®Œæˆ: {completed}æˆåŠŸ, {failed}å¤±è´¥")
    return descriptions

def convert_pptx_to_markdown(pptx_path: Path, output_path: Optional[Path] = None) -> Path:
    """
    è½¬æ¢ PPT åˆ° Markdownï¼ˆå¸¦å›¾ç‰‡æè¿°ï¼‰
    
    Args:
        pptx_path: PPT æ–‡ä»¶è·¯å¾„
        output_path: è¾“å‡º Markdown è·¯å¾„ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    if not pptx_path.exists():
        raise FileNotFoundError(f"PPT æ–‡ä»¶ä¸å­˜åœ¨: {pptx_path}")
    
    # ç¡®å®šè¾“å‡ºè·¯å¾„
    if output_path is None:
        output_path = pptx_path.with_suffix('.md')
    
    ensure_cache_dir()
    cache_path = get_cache_path(pptx_path)
    cache = load_cache(cache_path)
    
    print(f"ğŸ“„ å¤„ç†: {pptx_path.name}")
    print(f"   ğŸ’¾ ç¼“å­˜: {cache_path}")
    
    # æ­¥éª¤ 1: æå–å›¾ç‰‡
    extract_dir = CACHE_DIR / f"{pptx_path.stem}_images"
    extract_dir.mkdir(exist_ok=True)
    
    images = extract_images_from_pptx(pptx_path, extract_dir)
    if not images:
        print("   âš ï¸  æœªæ‰¾åˆ°å›¾ç‰‡")
    else:
        print(f"   ğŸ–¼ï¸  æ‰¾åˆ° {len(images)} å¼ å›¾ç‰‡")
    
    # æ­¥éª¤ 2: è¯†åˆ«å›¾ç‰‡
    if images:
        descriptions = process_images_batch(images, cache)
        save_cache(cache_path, cache)
    else:
        descriptions = {}
    
    # æ­¥éª¤ 3: è½¬æ¢ PPT åŸºç¡€å†…å®¹ï¼ˆä½¿ç”¨ markitdownï¼‰
    print("   ğŸ“ è½¬æ¢åŸºç¡€å†…å®¹...")
    try:
        from markitdown import MarkItDown
        md = MarkItDown()
        result = md.convert(str(pptx_path))
        content = result.text_content
    except ImportError:
        print("   âš ï¸  markitdown æœªå®‰è£…ï¼Œä»…ç”Ÿæˆå›¾ç‰‡æè¿°")
        content = f"# {pptx_path.stem}\n\n"
    
    # æ­¥éª¤ 4: æ·»åŠ å›¾ç‰‡æè¿°ï¼ˆä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ï¼‰
    if descriptions:
        print(f"   ğŸ”— æ•´åˆ {len(descriptions)} ä¸ªå›¾ç‰‡æè¿°...")
        
        # è½¬æ¢ descriptions key ä» imageX åˆ°åºå·
        num_to_desc = {}
        for img_id, desc in descriptions.items():
            match = re.search(r'\d+', img_id)
            if match:
                num_to_desc[int(match.group())] = desc
        
        # ä½¿ç”¨æ­£åˆ™æ›¿æ¢æ‰€æœ‰å›¾ç‰‡å¼•ç”¨
        img_pattern = r'!\[([^\]]*)\]\(([^)]+)\)'
        
        def replace_with_description(match):
            alt_text = match.group(1)
            img_path = match.group(2)
            full_match = match.group(0)
            
            # æå–å›¾ç‰‡ç¼–å·
            num_match = re.search(r'\d+', Path(img_path).stem)
            if not num_match:
                return full_match
            
            img_num = int(num_match.group())
            
            # æŸ¥æ‰¾å¯¹åº”çš„æè¿°
            if img_num in num_to_desc:
                desc = num_to_desc[img_num]
                desc_block = f"\n\n> ğŸ–¼ï¸ **å›¾ç‰‡æè¿°**: {desc[:150]}"
                if len(desc) > 150:
                    desc_block += "..."
                return full_match + desc_block
            
            return full_match
        
        content = re.sub(img_pattern, replace_with_description, content)
    
    # æ­¥éª¤ 5: ä¿å­˜
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(content)
    
    print(f"   âœ… è¾“å‡º: {output_path}")
    return output_path

def main():
    if len(sys.argv) < 2:
        print("ç”¨æ³•: pptx-to-md <input.pptx> [output.md]")
        print("")
        print("ç¯å¢ƒå˜é‡ (å¿…éœ€):")
        print("  LLM_API_URL    - LLM API ç«¯ç‚¹ (ä¾‹: https://api.openai.com/v1/chat/completions)")
        print("  LLM_MODEL      - æ¨¡å‹åç§° (ä¾‹: gpt-4o)")
        print("")
        print("ç¯å¢ƒå˜é‡ (å¯é€‰):")
        print("  LLM_API_KEY    - API å¯†é’¥")
        print("  MAX_WORKERS    - å¹¶å‘æ•° (é»˜è®¤: 3)")
        print("  CACHE_DIR      - ç¼“å­˜ç›®å½• (é»˜è®¤: /tmp/ppt_image_cache)")
        print("")
        print("è¯¦è§ .env.example é…ç½®ç¤ºä¾‹")
        sys.exit(1)
    
    input_path = Path(sys.argv[1])
    output_path = Path(sys.argv[2]) if len(sys.argv) > 2 else None
    
    print("=" * 60)
    print("ğŸš€ PPT è½¬ Markdownï¼ˆå¢å¼ºç‰ˆï¼‰")
    print(f"   æ¨¡å‹: {MODEL}")
    print(f"   å¹¶å‘: {MAX_WORKERS}")
    print("=" * 60)
    
    try:
        result_path = convert_pptx_to_markdown(input_path, output_path)
        print("\nğŸ‰ è½¬æ¢æˆåŠŸ!")
        print(f"   ğŸ“„ {result_path}")
    except Exception as e:
        print(f"\nâŒ è½¬æ¢å¤±è´¥: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
